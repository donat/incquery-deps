% ------------------------------------------------------------------------------
% 3 pages. Super figure which describes the entire solution. Choosing proper
% abstraction level for the %figure is essential. Every item on this figure will
% be an additional chapter in this paper.
% TODO:find a proper name for this chapter.
% ------------------------------------------------------------------------------
\chapter{Overview}


\section{System Architecture}
% My implementation.
As it was discussed previously one of the most effective  way of achieving
smooth upgrades is to discover incoming dependencies. For this I designed and
implemented a solution called \ptool{}. \picr{superfigure.pdf}{Overview of the
implemented system} \autoref{fig:superfigure.pdf} shows the overview of
it.

% Two implementation
This systems was implemented in two separate steps. My work at CERN covered an
implementation of a system which gives the ability to the users to query certain
parts of the source code for incoming dependencies. After my job was done I
created an extension which utilizes EMF-IncQuery to run faster, information-rich
and more extensive queries. Let's examine the elements of the figure one-by-one.


\subsection{System boundaries}
The system allows the users to examine the inter-project dependencies from the
development environment. It exposes two access points to access this
information. First the developer can initiate queries explicitly by selection
the target element from  he source code editor. The other option to check result
of the model queries (described later). This information is updated
automatically, every change in the development environment is reflected to the
result set. Both versions give visual representation of the result which the
developer can analyse and he can use it to make decisions what to and what not
to change in the source code.


\subsection{Repository management}
% Central repository management.
The first elements on the figure to discuss are the ''Source repository'' and
the 'Binary component repository' . They contain both the source code and the
build binary version of the internally developed softwares. These elements are
centrally managed and have a well-defined structure. The developers work on the
source code and if they finish one milestone they publish their improvements by
putting the compiled version into the binary repository through an automated
process.
The binary repository is the input for the dependency analysis.

\subsection{The server side}
The server side is a standalone Java application which runs on a Linux server.
It has two functionality: 1) it discovers and stores the structure of the
products and 2) provides an interface for serving dependency queries.

For discovering, the process listens if a new release happens in the repository.
If it happens the freshly added binaries (jar files) are passed to the byte code
analysis module which parses the file and discovers the contained structure and
the dependencies utilizing the Apache BCEL library.  The structure and the
dependencies are passed to the storage engine to store it. The storage engine
itself defines a set of operations to find, store and retrieve certain subset of
the model. The remote query interface also use this module to get the necessary
dependency information for the clients.


\subsection{The client side}
% Plugin
The client-side of the solution is an Eclipse plugin (or more precisely a set of
plugins) which gives the developer a convenient way to access to the dependency
information.

\subsubsection{Model load and direct queries}
The base of the plugin is the repository model loader. It provides a simple API
for accessing and querying the dependency information from the server. The
simple use-case for this, when the user directly asks the dependency information
from the Java source editor through UI contribution (marked as direct queries on
the figure).

\subsubsection{Workspace model creator}
The workspace model creator is module generating an EMF model describing the
state of the workspace. The generated model contains the loaded projects, the
contained packages, classes, methods, etc. The model also contains the
dependency structure between the elements (e.g. method calls, field accesses and
such). The information is gathered through the Eclipse Java Development Tools'
API. After the model is created it is incrementally maintained even through 
restarts.

\subsubsection{On-the-fly queries}
The pattern matcher module executes complex model queries on top of the acquired
models to provide dependency information. First it loads an EMF model from the
server describing the structure of all projects in the central repository.
Afterward the workspace model is loaded from the module described above. Both of
the models are loaded to the EMF-IncQuery engine. Through complex queries the two
models are joined and queried for the dependency information. With this more 
accurate and extensive dependency analysis can be achieved because this approach
takes into account the changes which were done in the workspace. The workspace 
model is updated upon every change of the workspace giving up-to-date feedback 
for the users while they are working on the development. 

\subsubsection{User Interface}
Both direct queries and the pattern matching module return the result in Eclipse 
views. After the result is evaluated, the user's responsibility to evaluate their
results and act accordingly.
\todo[inline]{UI szolgaltatasokat reszletesebben kifejteni, itemize}

\subsubsection{Implementation-specific requirements}
Using direct queries brings no limitations. The queries are simple remote method
calls and the result set is a relatively small data set which easy to store and
present on the Eclipse UI. On the other hand, the pattern matching solution is
far from being that easy, because in order to load make EMF-IncQuery work, the
entire repository has to be loaded. By default this model is a few hundred
megabytes sized in a serialized form. This implies that the implementation has to 
optimize this model without dropping useful information to make it loadable to 
the memory.

\section{Running Example: Service Provider Framework}\label{sect:spf}

To make the following chapters easier to understand, I am going to introduce a
simple use-case example. It is a design pattern called Service Provider
Framework. It is a practical application of the original Adapter pattern and it
was described in the famous book \emph{Effective Java} \cite{Bloch08}. This pattern
is the simplified version how the Java Database Connectivity (JDBC) works.

\subsection{Description}

\picr{exampleclasses.pdf}{Structure of Service Provider Framework design pattern}

You can see the structure of the pattern on
\autoref{fig:exampleclasses.pdf}. As of the packages it contains 3 major parts.
The \code{service} package contains the core pattern classes. The \code{impl}
and the \code{impl} packages are external users of the pattern and therefore
they are considered depending client libraries.

First let's discuss the pattern itself. The main goal for the patter is to
provide a registry of implementations for a desired service. This service is
described in the \code{Service} interface. The \code{Provider} interface is
serves as a factory instance; it has a single function to instantiate a new
Service object. The \code{Services} class has the role of the registry.
The service implementers register their implementation using the static
\code{registerProvider()} and \code{registerDefaultProvider()} methods. The
parameters the identifier string for the registered service and Provider
instance which will instantiate the Service instances. The clients will
instantiate the Service instance with the \code{newInstance()} function.
Depending the passed identifier string, the method will look up if a Provider
was registered with the same name, and if the answer is yes then it calls the
\code{Provider.newInstance()} and return its result.

The \code{DEFAULT\_PROVIDER\_NAME} is a static public field which can be used to
obtain the default Service implementation. The \code{AbstractService} is a
utility class which implements one of the function of the Service interface.

The \code{impl} package contains one possible implementation to use the
described pattern. The \code{BasicService} contains the implementation and the
\code{BasicProvider} is responsible for properly initializing and returning a
new instance of this version. The \code{BasicImplUtil} -- as its name implies --
holds utility classes which register the implementation in the Services class.

The \code{client} package holds one single \code{Main} class, which contains a
simple evaluation. It invokes the \code{Services.newInstance()} function passing
the default provider's name as an argument and invokes the \code{serviceA()} and
\code{serviceB()} function.

Although the example is fairly simple, the related source code can be found in
\autoref{examplesource}.

\subsection{Smooth upgrade challenges}
\todo[inline]{itemize-zal kiemelni a megoldando feldatokat, amelyeket majd megoldassz kesobb}
As it was described in the introduction we would like give the developers the
ability to check who is using the certain parts of the API. To present this this
pattern will be our use-case.

Imagine that the three packages are distributed into three different software
which have their individual developers. If somebody wants to change some parts
of the service without precisely knowing who is using which part of the code he
won't know how many dependant projects will be broken afterwards.

Let's see two examples. The responsible for the service package wants to modify
two parts of the classes: he wants to add a string parameter to the
\code{Service.serviceA()} method, and he wants to change the name of the
\code{Services.DEFAULT\_PROVIDER\_NAME} field. 

By using \ptool{} the developers are able to execute queries on any part of the
API. The result will be the primary starting point for considering what to do.
Three different outcome can happen: the selected part of the API doesn't have
dependencies, it has a few dependencies or it has got a lot and the developer's
decision depends on it. 

If there is no incoming dependency than any modifications can be done without
causing any compatibility issues. The developer can do whatever he wants.
If there is a lot of incoming dependency, than it means that the queried code
element is used by a lot of clients and any modifications would cause a build
error therefore backward compatibility has to be maintained. The third option is
that there are several dependencies. In this case the developer has to negotiate
with the clients how to proceed: he has to provide backward compatibility or the
clients has to align their code to the modifications.

\todo[inline]{eloreutalas a kesobbi fejezetekre, ahol a problemak megoldasa ki lesz fejtve}
