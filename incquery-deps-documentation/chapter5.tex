\chapter{Evaluation}
%------------------------------------------------------------------------------
%3-4 pages. Functional evaluation. Performance evaluation. 
%------------------------------------------------------------------------------

\section{Example revisited}

After showing the details of the implementation, let's see it in action.
We will use the tool to solve the problem described in \autoref{sect:spf}. We
will go though on every component and check what is happening there with the
example. 

Let's start with the repositories. In our example the source and the binary
repository contains three projects, each of them holds one package from the
example. the name of the projects are service, client and impl, the name of 
the jars files respectively service.jar, client.jar and impl.jar. For the sake
of simplicity we have only one version from each project. 

\subsection{On the server side}


Now let's move forward to the server process and the dependency analysis. As 
the binary projects appear in the repository, they get discovered by the 
server process. 

The first step in the process is the bytecode analysis. In this phase the
structure and the reference of the dependencies are discovered from the
binaries. As of the structure the analyzer finds 3 projects, 8 classes 2 fields
and 19 methods which get stored in the database.
The methods are composed of the 12 defined methods, 6 constructor (from all
non-interface classes) and 1 static initializer (which comes from the Services
class as it gives a default value to a static field).

The discovered dependencies are formatted and filtered in order not to have
internal or platform defined elements (dependencies pointing inside the
\code{java.*} package). For example let's see the \code{BasicImplUtil} class'
\code{registerImplementation()} method.
\begin{tabl}
{External references of the BasicImplUtil.registerImplementation() method}
{extrefs}{|p{.37\linewidth}|p{.37\linewidth}|p{.2\linewidth}|}
\hline
	\textbf{Binary format}						&  
	\textbf{Source format} 						& 
	\textbf{Type} 								\\
\hline
	impl/BasicProvider  						&
	impl.BasicProvider 							&
	ConstClass 									\\
\hline
	service/Services/registerProvider 
	\mbox{(Ljava/lang/String;}
	\mbox{Lservice/Provider;)V} 				& 
	\mbox{service.Services.registerProvider}
	\mbox{(java.lang.String,}
	\mbox{service.Provider):void} 				&
	\mbox{Constant} \mbox{MethodRef} 			\\
\hline
\end{tabl}
The first one is a 
\code{ConstantMethodRef} type reference. During the analysis they are first converted
The first one is a class reference the second one is a method reference.
The references are formatted to source format as it is shown in the second column.
Now the first references is eliminated from the list, because it points inside the 
\code{impl} project. However the second one does not, so it remains at its place.  

The dependency processor takes the remaining dependencies from the elements and 
searches the database if there is one. In case of our method reference, the processor
finds that there is a \code{registerProvider()} method defined with the same exact 
signature signature in the database. Because the reference was stored on the called 
method list, the processor creates a new method call dependency starting from 
\code{impl.BasicImplUtil} targeting \code{Services.registerProvider()}.

At this point the storage engine stores all the structure and the dependencies 
of the three products. The schema of the Oracle database implementation and the EMF
database was described in details previously. Now let's check how this data can be used 
on the client side.

\subsection{On the client side}

To continue our example, we will examine the view of the developer who knows
only about his own \code{service} project. If the \ptool{} is  installed in his
Eclipse, the EMF model -- shown on \autoref{fig:wsmodel.png}
\picr{wsmodel.png}{Workspace model containing Service package from the example}
-- is built and synchronized automatically. On the figure we can see, that the
structure of the project (left) corresponds to the content of the EMF model
(right). In the model view, you can also see that the in-project dependency
relationships are also registered.


Because the \code{client} and the \code{impl} projects have references to the
\code{service} project, both of them are loaded to the repository model. It has all the
element but with the compacted version, so no fully qualified names. But because
there is no overlapping names between the classes and method than the compacted
model does not introduce false-positive results.

Let's see how the developer checks the incoming dependencies through the
explicit queries.
\pic{incdeps.png}{Execute explicit queries from the source code editor}
\autoref{fig:incdeps.png} shows the context menu contribution for initiating an
explicit query. The query takes a few seconds to respond. The results are shown
in the viewer like on \autoref{fig:results.png}.
\picr{results.png}{Result of explicit queries}
 
\subsubsection{Model queries}

\paragraph{The compacted model}
The compacted version of the repository instance model (produced by the
dependency database synchronizer) contains all the three projects from the
repository. Part of the model is shown on
\autoref{fig:cp3modelinstance.pdf}\picr{cp3modelinstance.pdf}{Elements of the
compacted instance model} The structure contains the same projects and the same
classes as the original model, however the following information is eliminated.
\begin{itemize}
  \item The class objects has only their simple name left, the fully qualified name is left out.
There is no overlapping classes but they would have merged together if
there was any. Here the \code{client.Service} instance has only the name \code{Service}.
  \item The method signature was trimmed down int their names. All methods have only their 
name section of the signature, the parameter list, and the return value are dumped.    
The object representing the \code{service.Service.serviceB()} method contains only the \code{serviceB}
formation and has the reference on its container. 
  \item The only visible dependency on the figure points to the same elements as before.
  It defines a method call dependency which starts from the object representing the \code{client.Main.main()} method.
\end{itemize}

\paragraph{Query definitions}
One of the implemented queries is the one which retrieves the incoming method call 
dependencies from the model. The entire definition of this query is the following. 
\begin{lstlisting}[caption=Elements of the incoming method call dependency query]
private pattern joinProjects(repoProject : CP3Project, wsProject : WProject) = {
	CP3Project.name(repoProject, commonName);
	WProject.name(wsProject, commonName);
}

private pattern joinMethods(repoMethod : CP3Method, wsMethod : WMethod) = {
	CP3Project.classes.methods(repoProject, repoMethod);
	WProject.packageFragmentRoots.packageFragments.compilationUnits.types.
		methods(wsProject, wsMethod);
	find joinProjects(repoProject, wsProject);
	CP3Method.name(repoMethod, commonName);
	WMethod.name(wsMethod, commonName);
}

pattern incomingMethodCalls(wsTarget : WMethod, repoSource : CP3Method) = {
	find joinMethods(repoTarget, wsTarget);
	CP3Dep.from(dependency, repoSource);
	CP3Dep.to(dependency, repoTarget);
	CP3Dep.type(dependency, 1);
}
\end{lstlisting}

The first \code{joinProjects} query takes the projects from the workspace
instance model and connects them to the equivalent element in the compacted
repository instance model This join operation is based on the common \code{name}
property.

The \code{joinMethods} query does a similar thing, but in this case the methods
are joined together. The firs two instructions gathers the container projects'
reference. The third instruction calls the \code{joinProjects} query, so the 
results are filtered to only to the elements which exist both in the workspace
and in the repository instance model. The methods itself also joined by 
checking the common \code{name} property.

The last \code{incomingMethodCalls} query returns the required results. It lists
the methods present in the compacted workspace instance model and its incoming
dependency elements in pairs. To find them, first the \code{joinMethods} query
is executed. Then it filters to all the elements which are pointed by a
dependency instance. Note that \code{repoSource} argument is bound to the
repository dependency object.


\paragraph{Results of the query}
The result of the query is shown on \autoref{fig:modelqueryresult.png}.
\picr{modelqueryresult.png}{Result of the incoming method call dependency query}
The EMF-IncQuery engine found the desired dependency. It points to the compacted
repository element having a method call dependency to the workspace model's
equivalent.

In this example a dependency was found. In this case the developer 
has to decide whether he will maintain backward compatibility or he notifies the
maintainer of the \code{client} project that the API will break soon. If he tried
to modify the \code{serviceA()} method, we would have seen that there are no incoming
dependencies. It means that the developer can break the signature of the method as
he wishes. 

\section{Performance}
After the functionality was presented, let's check the overall performance of
the system. We have to check the individual components  that they can complete
their requirement as the input or usage scales up to a level of the production
applications. First we have to verify whether the dependency fast enough and can
process large number of Java binaries. The next one is to see if the explicit
queries are reasonably fast. The last yet the most important is to test to
performance of the EMF-IncQuery engine how big instance models can be loaded
into the EMF-IncQuery engine.

The jar files used as the sample data come from real-life applications; they are
operational softwares from the CERN controls systems. By using them we can
eliminate the problems of having badly generated homogeneous sample data.

\subsection{Dependency processing}
\label{sect:depproc}
The first element to measure is how the server process perform.
A measurement has to check how efficiency the bytecode analysis and the
dependency processing together works. Minimal preliminary requirement is that is
should be much faster then the source code of the software in  the repository
changes, because otherwise the dependency database will contain constantly
outdated information.

As a test I took all of the import binaries, started the server process as a
standalone Java application, initiated the dependency discovery on the subset of
the binaries and measured the statistics of the database and the speed of the
discovery. After the test has finished I erased the database and repeated the
test with a bigger and bigger subset of the jar files until the entire set was
analyzed. For the test I used a desktop computer with the following
specifications:
\begin{itemize}
  \item Processor: Intel Core i3-2120, 3,3 GHz,
  \item RAM: 8GB DDR3,
  \item OS: Windows 7 Enterprise, 64 bit, Service Pack 1,
  \item JVM: Oracle JDK 1.6.0\_27-b07.
\end{itemize}
The tests showed that on average one project contains 80 classes, 513 methods, 304 fields and 778 dependencies. 
The result of the test are shown on \autoref{fig:resanal.pdf}
\picr{resanal.pdf}{Result of dependency processing measurement}
The left chart shows how big generated EMF instance model depending on the
input jar size. It is clearly visible that the size linearly increases
as the input grows. Also the execution time (right chart) shows 
steady growth as the input size increases. Looking at the analysis
 of all (1300) jars, the average processing takes roughly $0,43$ seconds.
Knowing that even in large software repositories usually a few, maybe a few 
dozen releases happen, then having a system which analyses the differences within
a second is more than enough.

\subsection{Explicit queries}
\label{sect:explqueries}
Next question is how fast are the explicit queries. We want to see that the
results of a dependency query returns the data and visualizes the information
within reasonable time even when the result set bigger than usual. 

I set up the server process on the same machine, what I used for 
testing the dependency processing. I filled the database with with all the
dependency information. For measuring the speed of the queries used an average
virtual machine which from the CERN ecosystem which has similar specification
what is usually used for development:
\begin{itemize}
  \item Processor: Intel Xenon E5645, 2,4 GHz,
  \item RAM: 4GB DDR3,
  \item OS: Windows 7 Enterprise, 32 bit, Service Pack 1,
  \item JVM: Oracle JDK 1.6.0\_35-b10.
\end{itemize}
Because this machine runs on a virtualized infrastructure it can't be
considered as a steady platform; performance could change as the usage changes,
but it is strong enough for comfortable Java development. 

For the tests I chose a specific project which happens to be a widely-used
library by the other jars. During the measurement a query is initiated for 
the incoming dependencies for every single element of this project. 
I dropped the result where there was no dependencies and made a statistics, how 
the size of the result set affects on the response time.
The results are shown on \autoref{fig:expqmeasure.pdf}.
 \picfifty{expqmeasure.pdf}{Explicit queries measurement result}
As it turned out, that the response time does not depend on the size of the result,
it was returned roughly 200ms on every sample. This result means that utilizing 
dependency queries return the result in reasonable time. 


\subsection{Model queries}
Using EMF-IncQuery the question is always about the memory. Does my model fit into
the memory? If it does, and the engine is able to build the data structure on top
of it then we are in good position because afterward the update mechanism requires 
minimal resources. 

So, to test model queries I reused the sub-models from the previous tests (see
\autoref{sect:depproc}), and generated the correspondent compacted models from
them.
The description of the compacted model is at \autoref{sect:depdbsynch}. Normally
this a sub-instance model would load automatically from the server side, but now
we want to see how big model could be loaded at once to decide the applicability
of the component.

The used machine is the same virtual pc introduced in
\autoref{sect:explqueries}. Because this is a typical development machine, it is
also considered as a reference for the usability. In the measurement I opened an
Eclipse instance, loaded the patterns and made some modifications in the
workspace. I checked how fast the EMF-IncQuery initializes (including loading
the model), how much memory the entire Eclipse distribution consumes and
afterwards how fast can it react to the workspace modifications.
\picr{modelqueries.pdf}{Model queries measurement result}

The result of the measurement is shown on \autoref{fig:modelqueries.pdf}.
The query engine initialized relatively fast, even the model which holds 400
projects could load within a minute. Considering that this has to be done once
per session it is acceptable. Also with the memory allocation was linear with 
the model size, it didn't explode, grew linearly with the input size. 

No models bigger than 400 elements could fit into the memory (with 500 Eclipse
exited with \code{OutOfMemoryError}, even with the maximum allowed 1.5GiB heap size)
because of the 32 bit platform limitation. Applying 64 bit systems could extend
the range, but we have calibrate the solution to the target machines which
happens in this case.

Nevertheless, the target machine could load and hold an instance mode holding
structural and dependency information for 400 projects. It means that the number
of projects depending on the ones loaded into Eclipse lower than this number,
than the tool can work effectively. In the sample that is the case, the most
frequently used project has less than a hundred project-level users. For other
 software repositories it is presumably true as there should be no
 ''super-projects'' which are referenced from all other projects in the
 repository.
 
In conclusion we saw that all components have the desired effectiveness factor.
The system itself is usable, scalable with the input and a feasible solution for
achieving smooth upgrades. 
