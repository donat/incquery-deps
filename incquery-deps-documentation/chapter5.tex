\chapter{Performance Evaluation}
%------------------------------------------------------------------------------
%3-4 pages. Functional evaluation. Performance evaluation. 
%------------------------------------------------------------------------------

After the functionality was presented, let's check the overall performance of
the system. We have to check the individual components  that they can complete
their requirement as the input or usage scales up to a level of the production
applications. First we have to verify whether the dependency fast enough and can
process large number of Java binaries. The next one is to see if the explicit
queries are reasonably fast. The last yet the most important is to test to
performance of the EMF-IncQuery engine how big instance models can be loaded
into the EMF-IncQuery engine.

The jar files used as the sample data come from real-life applications; they are
operational softwares from the CERN controls systems. By using them we can
eliminate the problems of having badly generated homogeneous sample data.

\section{Dependency processing}
\label{sect:depproc}
The first element to measure is how the server process perform.
A measurement has to check how efficiency the bytecode analysis and the
dependency processing together works. Minimal preliminary requirement is that is
should be much faster then the source code of the software in  the repository
changes, because otherwise the dependency database will contain constantly
outdated information.

As a test I took all of the import binaries, started the server process as a
standalone Java application, initiated the dependency discovery on the subset of
the binaries and measured the statistics of the database and the speed of the
discovery. After the test has finished I erased the database and repeated the
test with a bigger and bigger subset of the jar files until the entire set was
analyzed. For the test I used a desktop computer with the following
specifications:
\begin{itemize}
  \item Processor: Intel Core i3-2120, 3,3 GHz,
  \item RAM: 8GB DDR3,
  \item OS: Windows 7 Enterprise, 64 bit, Service Pack 1,
  \item JVM: Oracle JDK 1.6.0\_27-b07.
\end{itemize}
The tests showed that on average one project contains 80 classes, 513 methods, 304 fields and 778 dependencies. 
The result of the test are shown on \autoref{fig:resanal.pdf}
\picr{resanal.pdf}{Result of dependency processing measurement}
The left chart shows how big generated EMF instance model depending on the
input jar size. It is clearly visible that the size linearly increases
as the input grows. Also the execution time (right chart) shows 
steady growth as the input size increases. Looking at the analysis
 of all (1300) jars, the average processing takes roughly $0,43$ seconds.
Knowing that even in large software repositories usually a few, maybe a few 
dozen releases happen, then having a system which analyses the differences within
a second is more than enough.

\section{Explicit queries}
\label{sect:explqueries}
Next question is how fast are the explicit queries. We want to see that the
results of a dependency query returns the data and visualizes the information
within reasonable time even when the result set bigger than usual. 

I set up the server process on the same machine, what I used for 
testing the dependency processing. I filled the database with with all the
dependency information. For measuring the speed of the queries used an average
virtual machine which from the CERN ecosystem which has similar specification
what is usually used for development:
\begin{itemize}
  \item Processor: Intel Xenon E5645, 2,4 GHz,
  \item RAM: 4GB DDR3,
  \item OS: Windows 7 Enterprise, 32 bit, Service Pack 1,
  \item JVM: Oracle JDK 1.6.0\_35-b10.
\end{itemize}
Because this machine runs on a virtualized infrastructure it can't be
considered as a steady platform; performance could change as the usage changes,
but it is strong enough for comfortable Java development. 

For the tests I chose a specific project which happens to be a widely-used
library by the other jars. During the measurement a query is initiated for 
the incoming dependencies for every single element of this project. 
I dropped the result where there was no dependencies and made a statistics, how 
the size of the result set affects on the response time.
The results are shown on \autoref{fig:expqmeasure.pdf}.
 \picfifty{expqmeasure.pdf}{Explicit queries measurement result}
As it turned out, that the response time does not depend on the size of the result,
it was returned roughly 200ms on every sample. 

Important, that  the measurement does not include the network delay. Also the 
returned  returned results contain dependency information for one single element. 
It is acceptable time for retrieving dependency information. 


\section{Model queries}
Using EMF-IncQuery the question is always about the memory. Does my model fit into
the memory? If it does, and the engine is able to build the data structure on top
of it then we are in good position because afterward the update mechanism requires 
minimal resources. 

So, to test model queries I reused the sub-models from the previous tests (see
\autoref{sect:depproc}), and generated the correspondent compacted models from
them.
The description of the compacted model is at \autoref{sect:depdbsynch}. Normally
this a sub-instance model would load automatically from the server side, but now
we want to see how big model could be loaded at once to decide the applicability
of the component.

The used machine is the same virtual pc introduced in
\autoref{sect:explqueries}. Because this is a typical development machine, it is
also considered as a reference for the usability. In the measurement I opened an
Eclipse instance, loaded the patterns and made some modifications in the
workspace. I checked how fast the EMF-IncQuery initializes (including loading
the model), how much memory the entire Eclipse distribution consumes and
afterwards how fast can it react to the workspace modifications.
\picr{modelqueries.pdf}{Model queries measurement result}

The result of the measurement is shown on \autoref{fig:modelqueries.pdf}.
The query engine initialized relatively fast, even the model which holds 400
projects could load within a minute. Considering that this has to be done once
per session it is acceptable. Also with the memory allocation was linear with 
the model size, it didn't explode, grew linearly with the input size. 

No models bigger than 400 elements could fit into the memory (with 500 Eclipse
exited with \code{OutOfMemoryError}, even with the maximum allowed 1.5GiB heap size)
because of the 32 bit platform limitation. Applying 64 bit systems could extend
the range, but we have calibrate the solution to the target machines which
happens in this case.

Nevertheless, the target machine could load and hold an instance mode holding
structural and dependency information for 400 projects. It means that the number
of projects depending on the ones loaded into Eclipse lower than this number,
than the tool can work effectively. In the sample that is the case, the most
frequently used project has less than a hundred project-level users. For other
 software repositories it is presumably true as there should be no
 ''super-projects'' which are referenced from all other projects in the
 repository.
 

\section{Performance evaluation conclusions}
\todo[inline]{attekinto osszesites arrol, hogy az egyes funkciok hogyan egeszitik ki egymast teljesitmeny szempontjabol}

In conclusion we saw that all components have the desired effectiveness factor.
The system itself is usable, scalable with the input and a feasible solution for
achieving smooth upgrades. 
